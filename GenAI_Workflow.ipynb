{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a322299-a686-4e0e-83be-c304816d38e5",
   "metadata": {},
   "source": [
    "# Summary Project: Recipe Database\n",
    "### This notebook gives overview of the processing as well as usage of GenAI in the processing of the recipe data. For this, the steps will be:\n",
    "\n",
    "1. Translation of the Data\n",
    "2. Tagging the data by ingredients (and checking the model responses)\n",
    "3. Creating a new feature to identify difficulty\n",
    "4. Giving the database an index\n",
    "5. Evaluation of the model\n",
    "6. Teaching the model the output wanted\n",
    "7. Creating a chatbot to be able to query through the recipes also using the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3b4f0c-2e96-4aff-8af5-076b8aa2d7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Set your OpenAI API key\n",
    "#os.environ[\"OPENAI_API_KEY\"]=  'This key has been revoked'\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f68d5b-5f60-4b28-bba5-e803f6c3d9c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Data pre-processing: Translation\n",
    "### Since the data I gathered consists of one larger english recipe database as well as one smaller german one, translating one of them is needed for consistency\n",
    "\n",
    "I tried doing this with ChatGPT API still, since Local LLMs don't work well with my personal setup (low CPU/GPU etc.)\n",
    "Admittedly, this was quite costly, since my recipe CSV in German consisted of roughly \n",
    "2300 000 Tokens that had to be translated. Originally the plan was to use a translation with GPT4, however, the model turned out to be simply too expensive for a trial and error period as it would cost multiple $ for a try of translation. This, however, means that some of the translations that were given might be some mistakes, on first glance, however, I was not able to find any severe mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2666d4a2-dd9c-4005-a000-d13cfa5c66d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "50 parts done\n",
      "Translated file saved to recipes_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = './data/rezepte.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to translate text using the ChatGPT API\n",
    "def translate_text(text, source_lang='de', target_lang='en'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a translator from {source_lang} to {target_lang}.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    #return response['choices'][0]['message']['content']\n",
    "\n",
    "# Translate the columns\n",
    "translated_data = {\"Name\": [], \"Ingredients\": [], \"Instructions\": []}\n",
    "\n",
    "for column in df.columns:\n",
    "    counter = 0\n",
    "    for text in df[column]:\n",
    "        counter += 1\n",
    "        translated_text = translate_text(text)\n",
    "        translated_data[column].append(translated_text)\n",
    "        if(counter > 50):\n",
    "            print(\"50 parts done\")\n",
    "            counter = 0\n",
    "\n",
    "# Create a new DataFrame with the translated data\n",
    "df_translated = pd.DataFrame(translated_data)\n",
    "\n",
    "# Save the translated DataFrame to a new CSV file\n",
    "translated_file_path = 'recipes_2.csv'\n",
    "df_translated.to_csv(translated_file_path, index=False)\n",
    "\n",
    "print(f\"Translated file saved to {translated_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de67f0-72e2-467d-9541-2674fd9e3c7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Checking the model for needed headwords and tagging the recipes for important characteristics\n",
    "### To be able to retrieve the right information, the recipes need to be tagged for important factors like \"vegan\", \"gluten-free\" etc.. These are simple tags related to the ingredients of the recipe. More in-depth tagging for information as well as feature engineering will be done in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f50a26-8e79-4f00-8e80-1725e3f82c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated CSV file with gluten-free tags is saved as tagged_vegan_recipes_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to tag rows as gluten-free\n",
    "def tag_gluten_free(csv_path, ingredient_list):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Create a new column \"gluten-free\" initialized with False\n",
    "    df['gluten-free'] = False\n",
    "    \n",
    "    # Iterate over each row to check ingredients\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if any ingredient from the list is in the row's ingredients\n",
    "        if not any(ingredient in row['Ingredients'].lower() for ingredient in ingredient_list):\n",
    "            df.at[index, 'gluten-free'] = True  \n",
    "    \n",
    "    # Save the updated CSV to a new file\n",
    "    new_csv_path = 'tagged_' + csv_path\n",
    "    df.to_csv(new_csv_path, index=False)\n",
    "    \n",
    "    return new_csv_path\n",
    "\n",
    "# Using a generated list of gluten ingredients\n",
    "gluten_ingredients = [\n",
    "    'wheat', 'barley', 'rye', 'oats', 'spelt', 'kamut', 'triticale', \n",
    "    'bulgur', 'couscous', 'farina', 'semolina', 'durum', 'einkorn', \n",
    "    'emmer', 'farro', 'graham', 'matzo', 'seitan', 'wheat bran', \n",
    "    'wheat germ', 'wheat starch', 'malt', 'malt extract', 'malt syrup', \n",
    "    'malt flavoring', 'malt vinegar', 'brewer’s yeast', 'hydrolyzed wheat protein', \n",
    "    'hydrolyzed wheat starch', 'modified wheat starch', 'modified food starch (when derived from wheat)',\n",
    "    'soy sauce (unless labeled gluten-free)', 'teriyaki sauce', 'imitation crab meat', \n",
    "    'starch (unless labeled gluten-free)', 'vegetable gum', 'vegetable starch', \n",
    "    'atta', 'bran', 'bread flour', 'cake flour', 'durum flour', 'enriched flour', \n",
    "    'farina', 'gluten flour', 'graham flour', 'high-gluten flour', 'high-protein flour', \n",
    "    'matzoh meal', 'self-rising flour', 'vital wheat gluten', 'whole wheat flour'\n",
    "]\n",
    "\n",
    "# Path to the input CSV file\n",
    "csv_file_path = 'vegan_recipes_2.csv'\n",
    "\n",
    "# Call the function and tag the CSV file\n",
    "tagged_csv_path = tag_gluten_free(csv_file_path, gluten_ingredients)\n",
    "\n",
    "print(f'The updated CSV file with gluten-free tags is saved as {tagged_csv_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f83f1be-358e-4204-a5a6-36b025b4d456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated CSV file with gluten-free tags is saved as vegan_recipes_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to tag rows as gluten-free\n",
    "def tag_vegan(csv_path, ingredient_list):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Create a new column \"gluten-free\" initialized with False\n",
    "    df['vegan'] = False\n",
    "    \n",
    "    # Iterate over each row to check ingredients\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if any ingredient from the list is in the row's ingredients\n",
    "        if not any(ingredient in row['Ingredients'].lower() for ingredient in ingredient_list):\n",
    "            df.at[index, 'vegan'] = True  \n",
    "    \n",
    "    # Save the updated CSV to a new file\n",
    "    new_csv_path = 'vegan_' + csv_path\n",
    "    df.to_csv(new_csv_path, index=False)\n",
    "    \n",
    "    return new_csv_path\n",
    "\n",
    "# Using a generated list of gluten ingredients\n",
    "non_vegan_ingredients = [\n",
    "    'meat', 'beef', 'pork', 'lamb', 'chicken', 'turkey', 'fish', 'seafood',\n",
    "    'crustaceans', 'shellfish', 'shrimp', 'crab', 'lobster', 'mussels', 'clams',\n",
    "    'oysters', 'gelatin', 'collagen', 'bone broth', 'beef broth', 'chicken broth',\n",
    "    'pork broth', 'fish sauce', 'anchovy paste', 'anchovies', 'honey', 'beeswax',\n",
    "    'propolis', 'royal jelly', 'eggs', 'egg whites', 'egg yolks', 'albumin', 'casein',\n",
    "    'whey', 'lactose', 'milk', 'butter', 'cream', 'cheese', 'yogurt', 'ghee',\n",
    "    'buttermilk', 'custard', 'milk chocolate', 'milk powder', 'clarified butter',\n",
    "    'whey protein', 'caseinates', 'lactalbumin', 'lactoglobulin', 'lard', 'suet',\n",
    "    'tallow', 'shortening (animal fat)', 'shellac', 'carmine', 'cochineal',\n",
    "    'isenglass', 'vitamin d3 (from lanolin)', 'omega-3 fatty acids (from fish)',\n",
    "    'lecithin (unless specified as soy lecithin)', 'mono- and diglycerides (unless specified as plant-derived)'\n",
    "]\n",
    "\n",
    "# Path to the input CSV file\n",
    "csv_file_path = 'recipes_2.csv'\n",
    "\n",
    "# Call the function and tag the CSV file\n",
    "tagged_csv_path = tag_vegan(csv_file_path, non_vegan_ingredients)\n",
    "\n",
    "print(f'The updated CSV file with gluten-free tags is saved as {tagged_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0321219f-6713-41a3-a297-22c9a2747b28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated CSV file with gluten-free tags is saved as all_tagstagged_vegan_recipes_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to tag rows as gluten-free\n",
    "def tag_vegetarian(csv_path, ingredient_list):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Create a new column \"gluten-free\" initialized with False\n",
    "    df['vegetarian'] = False\n",
    "    \n",
    "    # Iterate over each row to check ingredients\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if any ingredient from the list is in the row's ingredients\n",
    "        if not any(ingredient in row['Ingredients'].lower() for ingredient in ingredient_list):\n",
    "            df.at[index, 'vegetarian'] = True  \n",
    "    \n",
    "    # Save the updated CSV to a new file\n",
    "    new_csv_path = 'all_tags' + csv_path\n",
    "    df.to_csv(new_csv_path, index=False)\n",
    "    \n",
    "    return new_csv_path\n",
    "\n",
    "# Using a generated list of gluten ingredients\n",
    "non_vegetarian_ingredients = [\n",
    "    'meat', 'beef', 'pork', 'lamb', 'chicken', 'turkey', 'duck', 'fish', 'seafood',\n",
    "    'crustaceans', 'shellfish', 'shrimp', 'crab', 'lobster', 'mussels', 'clams',\n",
    "    'oysters', 'gelatin', 'collagen', 'bone broth', 'beef broth', 'chicken broth',\n",
    "    'pork broth', 'fish broth', 'fish sauce', 'anchovy paste', 'anchovies', \n",
    "    'animal fat', 'lard', 'suet', 'tallow', 'shortening (animal fat)', 'isenglass',\n",
    "    'carmine', 'cochineal', 'shellac', 'rennet', 'vitamin d3 (from lanolin)',\n",
    "    'omega-3 fatty acids (from fish)', 'mono- and diglycerides (unless specified as plant-derived)',\n",
    "    'lecithin (unless specified as soy lecithin)'\n",
    "]\n",
    "\n",
    "# Path to the input CSV file\n",
    "csv_file_path = 'tagged_vegan_recipes_2.csv'\n",
    "\n",
    "# Call the function and tag the CSV file\n",
    "tagged_csv_path = tag_vegetarian(csv_file_path, non_vegetarian_ingredients)\n",
    "\n",
    "print(f'The updated CSV file with gluten-free tags is saved as {tagged_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c608cd7-7b25-4aef-8585-f38946518195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to tag rows as gluten-free\n",
    "def tag_fruit(csv_path, ingredient_list):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Create a new column \"gluten-free\" initialized with False\n",
    "    df['fruit'] = False\n",
    "    \n",
    "    # Iterate over each row to check ingredients\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if any ingredient from the list is in the row's ingredients\n",
    "        if not any(ingredient in row['Ingredients'].lower() for ingredient in ingredient_list):\n",
    "            df.at[index, 'fruit'] = True  \n",
    "    \n",
    "    # Save the updated CSV to a new file\n",
    "    new_csv_path = 'finished' + csv_path\n",
    "    df.to_csv(new_csv_path, index=False)\n",
    "    \n",
    "    return new_csv_path\n",
    "\n",
    "# Using a generated list of gluten ingredients\n",
    "fruits = [\n",
    "    'apple', 'apricot', 'banana', 'blackberry', 'blueberry', 'cantaloupe', 'cherry',\n",
    "    'coconut', 'date', 'dragon fruit', 'fig', 'grape', 'grapefruit', 'kiwi', 'lemon',\n",
    "    'lime', 'mango', 'nectarine', 'orange', 'papaya', 'peach', 'pear', 'pineapple',\n",
    "    'plum', 'pomegranate', 'raspberry', 'strawberry', 'tangerine', 'watermelon',\n",
    "    'passion fruit', 'persimmon', 'quince', 'star fruit', 'guava', 'honeydew', 'clementine',\n",
    "    'kumquat', 'loquat', 'lychee', 'mandarin', 'mulberry', 'olive', 'prickly pear', 'sapodilla'\n",
    "]\n",
    "\n",
    "# Path to the input CSV file\n",
    "csv_file_path = 'all_tagstagged_vegan_recipes_2.csv'\n",
    "\n",
    "# Call the function and tag the CSV file\n",
    "tagged_csv_path = tag_fruit(csv_file_path, fruits)\n",
    "\n",
    "print(f'The updated CSV file with gluten-free tags is saved as {tagged_csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98ef52-1171-48e3-ad89-1ad9833f19de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. New tag: difficulty\n",
    "### For this specific tag, I asked the LLM to combine three components for each recipe: \n",
    "1. The number of ingredients\n",
    "2. the amount of time needed to finish\n",
    "3. the number of utensils needed\n",
    "### This should be converted into a difficulty scale from 1-5, 1 being very easy and 5 being very hard. This process had to be split down into multiple functions, since otherwise the runningtime as well as the mistakes made were too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524d613-13b7-4ea3-abcc-73e503aa9d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the API to find all the components needed for the calculation\n",
    "def find_components(row):\n",
    "    text = f\"\"\"\n",
    "    Ingredients: {row['Ingredients']}\n",
    "    Utensils: {row['Instructions'], row[\"Ingredients\"]}\n",
    "    Cooking Time: {row['Instructions']}\n",
    "    Instructions: {row['Instructions']}\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You will be given a row of a CSV file. You should read all the columns in this row to find the following three components:\n",
    "        1. how many ingredients I need\n",
    "        2. how many utensils are mentioned\n",
    "        3. how long it takes to cook the recipe\n",
    "    To count the ingredients needed, you need to count each element that is listed in the column \"Ingredients\" and see if it is \n",
    "    edible and therefore considered food. If elements in this column are not edible, they might belong to the category utensils. \n",
    "    To find all utensils you need to check the ingredients as well as the instructions listed for each recipe to see which utensils \n",
    "    are needed. Examples for utensils would be pans, spoons, knifes, baking dishes, grater, mixer etc.\n",
    "    If the word \"cut\" is mentioned in the instructions, but no knife has been listed as utensils, calculate Utensils += 1.\n",
    "    The cooking time should me mentioned in the column \"Instructions\". If there is no cocking time, write \"None\".\n",
    "    As Output please list the component and only the number as integer or time.\n",
    "    \n",
    "    Here is an example:\n",
    "    Baked Apples, Mango, Spoon, Pan, Ananas, Baking dish. Cut all fruit and put into baking dish.\n",
    "    \n",
    "    Calcuation:\n",
    "    Ingredients: 3, Utensils: 3, Cooking Time: None\n",
    "    If the word \"cut\" is mentioned in the instructions, but no knife has been listed as utensils then Utensils += 1.\n",
    "    \n",
    "    Output:\n",
    "    Ingredients: 3, Utensils: 4, Cooking Time: None\n",
    "    \n",
    "    Here is the row data: {text}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You find componenents of recipes\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    #print(response.choices[0].message.content.strip())\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Read the input CSV file\n",
    "input_file_path = 'tagged_recipes.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Create a new column in the DataFrame to store the tags\n",
    "components = []\n",
    "\n",
    "counter = 0\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    counter += 1\n",
    "    if(counter > 50):\n",
    "            print(\"50 rows done\")\n",
    "            counter = 0\n",
    "    # Apply the function to the row\n",
    "    component = find_components(row)\n",
    "    # Append the result to the list\n",
    "    components.append(component)\n",
    "\n",
    "print(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39ce79-ccef-4da3-8e1a-e2ef4e485455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking the components: they are not greatly formatted in the way they are supposed to\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e4bf7231-ba2b-4f00-8383-6812be22de60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 elements done\n",
      "50 elements done\n",
      "50 elements done\n",
      "50 elements done\n",
      "50 elements done\n",
      "50 elements done\n",
      "[10, 22, 11, 19, 18, 18, 22, 17, 23, 12, 18, 31, 22, 25, 18, 6, 20, 8, 14, 10, 9, 12, 17, 20, 6, 8, 15, 15, 9, 22, 8, 9, 33, 13, 15, 27, 16, 16, 25, 13, 14, 14, 22, 18, 37, 15, 25, 8, 5, 19, 9, 19, 25, 17, 6, 25, 24, 12, 9, 34, 17, 12, 11, 15, 9, 13, 32, 9, 12, 11, 9, 18, 5, 9, 24, 20, 17, 10, 23, 8, 15, 14, 34, 20, 11, 19, 14, 26, 17, 14, 22, 6, 18, 8, 10, 9, 17, 10, 13, 10, 25, 12, 8, 10, 9, 20, 7, 14, 24, 8, 27, 24, 21, 12, 31, 9, 12, 11, 26, 21, 19, 11, 20, 22, 16, 23, 23, 10, 7, 13, 9, 28, 25, 7, 29, 7, 23, 10, 19, 37, 7, 19, 39, 14, 17, 15, 15, 19, 13, 14, 22, 25, 4, 11, 17, 17, 18, 12, 16, 13, 21, 11, 15, 7, 20, 10, 9, 14, 14, 11, 14, 14, 27, 20, 15, 12, 24, 19, 18, 17, 13, 12, 33, 26, 23, 17, 15, 10, 38, 11, 23, 11, 17, 13, 23, 22, 9, 27, 18, 17, 18, 21, 13, 25, 30, 21, 18, 11, 20, 22, 17, 20, 16, 32, 11, 24, 16, 11, 18, 17, 10, 18, 25, 20, 23, 19, 8, 12, 19, 23, 16, 28, 11, 24, 14, 13, 24, 17, 17, 15, 36, 17, 8, 25, 54, 10, 19, 5, 15, 14, 25, 19, 32, 18, 5, 7, 22, 14, 29, 18, 19, 21, 11, 15, 12, 14, 26, 11, 22, 19, 10, 17, 21, 19, 12, 11, 10, 20, 21, 8, 13, 11, 17, 16, 32, 18, 16, 6, 17, 9, 14, 25, 6, 10, 5, 12, 15, 27, 18, 24, 17, 10, 17, 14, 18, 30, 11, 64, 18, 13, 16, 19, 10, 13, 26, 14, 15, 12, 14, 18, 13, 9, 17]\n"
     ]
    }
   ],
   "source": [
    "# Function to create calculate the difficulty based on the calculation given\n",
    "# it was decided that the utensils should have the highest weight in the calculation\n",
    "def calculate_difficulty(string):\n",
    "    prompt = f\"\"\"\n",
    "    You will be given a list. For each string in the list, you should calculate a \"difficulty value\".\n",
    "    You need to extract numbers for three variables: \"Ingredients\", \"Utensils\" and \"Cooking Time\".\n",
    "    \n",
    "    For this, you need to follow these instructions:\n",
    "    for the variable \"cooking level\" the following structure is applied:\n",
    "    Cooking level 1: cooking time <= 30min\n",
    "    cooking level 2: 30min < cooking time <= 60min\n",
    "    cooking level 3: 60min < cooking time <= 90min\n",
    "    cooking level 4: 90min < cooking time <= 120min\n",
    "    cooking level 5: everything longer than 120min or 2 hours\n",
    "    \n",
    "    difficulty_value = (number of ingredients) + (number of utensils * 2) + (cooking level)\n",
    "    If one variable is missing, use 0 as the number.\n",
    "    If all variables are missing, write \"None\" as output.\n",
    "    \n",
    "    Here is an example for the input and output:\n",
    "    Input: \n",
    "    Ingredients: 7, Utensils: 3, Cooking time: 30min\n",
    "    \n",
    "    Calculation: \n",
    "    7 + (3*2) + 1 = 14\n",
    "    \n",
    "    As output you should give only the calculated difficulty value:\n",
    "    difficulty_value = 14\n",
    "    \n",
    "    Here is the string data: {string}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You define a difficulty value based on a given calculation\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    response_text = response.choices[0].message.content.strip()\n",
    "    match = re.findall(r'\\d+', response_text)\n",
    "    if match:\n",
    "        last_number = match[-1]\n",
    "    return int(last_number)\n",
    "\n",
    "difficulty_values = []\n",
    "\n",
    "counter = 0\n",
    "# Iterate over each element in the list\n",
    "for element in components:\n",
    "    counter += 1\n",
    "    if(counter > 50):\n",
    "        print(\"50 elements done\")\n",
    "        counter = 0\n",
    "    # Apply the function to the element\n",
    "    difficulty_value = calculate_difficulty(element)\n",
    "    # Append the result to the list\n",
    "    difficulty_values.append(difficulty_value)\n",
    "\n",
    "print(difficulty_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bbd38068-f7c5-45e7-9b15-e13db0378350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 64\n"
     ]
    }
   ],
   "source": [
    "lowest_value = min(difficulty_values)\n",
    "highest_value = max(difficulty_values)\n",
    "print(lowest_value, highest_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318cf93-d335-4d49-9c1c-bff38fad7624",
   "metadata": {},
   "source": [
    "Now we only need to make grades out of the values calculated. For this they were standardized to a scale of 1-5 were 1 is very easy and 5 is very hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a2fc1683-b042-4057-ae24-3e201ed261e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardize_to_grades(values, old_min=4, old_max=64, new_min=1, new_max=5):\n",
    "    # Ensure the input values are within the expected range\n",
    "    if not all(old_min <= v <= old_max for v in values):\n",
    "        raise ValueError(f\"All values should be between {old_min} and {old_max}\")\n",
    "\n",
    "    standardized_values = []\n",
    "    for value in values:\n",
    "        normalized = (value - old_min) / (old_max - old_min)\n",
    "        standardized = new_min + normalized * (new_max - new_min)\n",
    "        standardized_values.append(round(standardized))\n",
    "\n",
    "    return standardized_values\n",
    "\n",
    "# Example usage\n",
    "grades = standardize_to_grades(difficulty_values)\n",
    "#print(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6ebee-095e-4043-ac8a-6bb5b86d7718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grade_to_word(grade):\n",
    "    if grade == 1:\n",
    "        return 'Very easy'\n",
    "    elif grade == 2:\n",
    "        return 'Easy'\n",
    "    elif grade == 3:\n",
    "        return 'Medium'\n",
    "    elif grade == 4:\n",
    "        return 'Difficult'\n",
    "    else:\n",
    "        return 'Very difficult'\n",
    "\n",
    "word_grades = [grade_to_word(grade) for grade in grades]\n",
    "\n",
    "#print(word_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cb483f5a-b071-49d6-961a-01e921196c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the difficulty values to the CSV file using Dataframes\n",
    "input_file_path = 'difficulty_recipes.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "df['difficulty_values'] = grades\n",
    "df['difficulty_words'] = word_grades\n",
    "df = df.rename(columns={'fruit': 'fruitless'})\n",
    "del df[\"Difficulty\"]\n",
    "df\n",
    "\n",
    "#Saving the finished recipe file\n",
    "output_file_path = 'data/finished_recipes.csv'\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03660743-b8b2-403a-8dfa-c3db6f1b8ffb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Indexing\n",
    "### To be able to efficiently work with the data as well as use it for the chatbot, it needs to be indexed. For this, VectorStoreIndex was used (mostly using the code provided during the lessons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64374f21-578b-46d8-a9e8-f05a3e787557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Llamaindex LLMs\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "## Llamaindex readers\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "## LlamaIndex Index Types\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "\n",
    "## LlamaIndex Context Managers\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core import Settings\n",
    "\n",
    "## LlamaIndex Templates\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "## LlamaIndex Agents\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "## LlamaIndex Callbacks\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.callbacks import LlamaDebugHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760e4290-cd1b-4d0c-9a21-4ca8e51cd9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=\"gpt-3.5-turbo\"\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "Settings.llm = OpenAI(temperature=0, \n",
    "                      model=model, \n",
    "                      #max_tokens=512\n",
    "                      PRESENCE_PENALTY=-2,\n",
    "                      TOP_P=1,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28fc7eb0-a8b7-4ca0-8484-f3126e6a4cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir: C:\\Users\\carla\\Desktop\\Universität\\Master Digital Humanities\\Semester 4\\GenAI\\GenAI_Project\n",
      "Files in C:\\Users\\carla\\Desktop\\Universität\\Master Digital Humanities\\Semester 4\\GenAI\\GenAI_Project\\Data\n",
      ".ipynb_checkpoints\n",
      "finished_recipes.csv\n",
      "recipes.txt\n"
     ]
    }
   ],
   "source": [
    "DOCS_DIR = os.path.join(os.getcwd(), \"Data\")\n",
    "PERSIST_DIR = os.path.join(os.getcwd(), \"Index\")\n",
    "\n",
    "print(f\"Current dir: {os.getcwd()}\")\n",
    "\n",
    "if not os.path.exists(DOCS_DIR):\n",
    "  os.mkdir(DOCS_DIR)\n",
    "docs = os.listdir(DOCS_DIR)\n",
    "docs = [d for d in docs]\n",
    "docs.sort()\n",
    "print(f\"Files in {DOCS_DIR}\")\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "994ec152-f485-4d76-88e3-09a2d4470eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[f\"{DOCS_DIR}/finished_recipes.csv\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "915833d8-18c4-4d19-9449-7a9cfbdbed7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_retrieve_index(index_path, docs_path, index_type):\n",
    "    if not os.path.exists(index_path):\n",
    "        print(f\"Creating Directory {index_path}\")\n",
    "        os.mkdir(index_path)\n",
    "    if os.listdir(index_path) == []:\n",
    "        print(\"Loading Documents...\")\n",
    "        required_exts = [\".txt\"]\n",
    "        documents = SimpleDirectoryReader(required_exts=required_exts, \n",
    "                                          input_dir=docs_path).load_data()\n",
    "        print(\"Creating Index...\")\n",
    "        index = index_type.from_documents(documents,\n",
    "                                          show_progress=True,\n",
    "                                          )\n",
    "        print(\"Persisting Index...\")\n",
    "        index.storage_context.persist(persist_dir=index_path)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"Reading from Index...\")\n",
    "        index = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=index_path))\n",
    "        print(\"Done!\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c3221cf-e04f-4018-95d6-6742442e1d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from Index...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "VECTORINDEXDIR = PERSIST_DIR + 'VectorStoreIndex'\n",
    "vectorstoreindex = create_retrieve_index(VECTORINDEXDIR, DOCS_DIR, VectorStoreIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105879e-e064-4462-883c-dcd47c3ce347",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5. Evaluation of the model\n",
    "### Since there were scandals on AI giving out recipes that are dangerous to humans, I would also like to include a quick \"health check\" in the processing of the model, in order to make sure that the recipes of the output are actually ones that are reliable.\n",
    "Originally this would have been necessary if I were to use the other recipe database from kaggle as well. Since I am only using the one from the German Bundesverband der Verbraucherinitiative I assume that the recipes have no health hazards in them. I still wanted to provide the basic structure for a pipeline like this. Additionally, it would usually be necessary to evaluate your model performance. However, since the only model I am personally able to run is via the ChatGPT API it is hard for me to do so. I still wanted to mention, that this step would usually be necessary for a complete workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa732e-200b-4bb6-9781-c5f402fc85b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Instructions on output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cb472f5-7611-4179-a3b4-92c52b427bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.prompts.base import ChatPromptTemplate\n",
    "\n",
    "# text qa prompt\n",
    "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n",
    "    content=(\n",
    "        \"You are an expert Q&A system that is trusted around the world.\\n\"\n",
    "        \"Always answer the query using the provided context information, \"\n",
    "        \"and not prior knowledge.\\n\"\n",
    "        \"Some rules to follow:\\n\"\n",
    "        \"1. Never directly reference the given context in your answer.\\n\"\n",
    "        \"2. Avoid statements like 'Based on the context, ...' or \"\n",
    "        \"'The context information ...' or anything along \"\n",
    "        \"those lines. \\n\"\n",
    "        \"3. Follow the structure provided for answering strictly,\"\n",
    "        \"which means only answer in this structure.\\n\"\n",
    "        \"4. Always give all the information about the recipe you can find in the dataset, including every column.\\n\"\n",
    "    ),\n",
    "    role=MessageRole.SYSTEM,\n",
    ")\n",
    "\n",
    "TEXT_QA_PROMPT_TMPL_MSGS = [\n",
    "    TEXT_QA_SYSTEM_PROMPT,\n",
    "    ChatMessage(\n",
    "        content=(\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge, \"\n",
    "            \"answer the query strictly always following the structure.\\n\"\n",
    "            \"Answer all questions only the structure given at answer. \\n\"\n",
    "            \"\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer:\\n\"\n",
    "            \"Sure! Here is the recipe:\\n\"\n",
    "            \"Cooking time: [list cooking time if available]\\n\"\n",
    "            \"Ingredients: [list ingredients here]\\n\"\n",
    "            \"Instructions: [list instructions here]\\n\"\n",
    "            \"Additional info on this recipe is [additional information]\\n.\"\n",
    "            \"\\n\"\n",
    "        ),\n",
    "        role=MessageRole.USER,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "CHAT_TEXT_QA_PROMPT = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5f4f7-502d-48a2-bd3b-b98edfe03b71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. Lauching Chatbot for queries\n",
    "### The final step of this project is to be able to chat with the recipe database. For this, the chatbot was already provided a specific structure in which it is supposed to answer in step 5, after indexing the database for this. Now, you can ask the chatbot for specific types of recipes, including tags for gluten-free, vegan, vegetarian, fruitless as well as a difficulty level, which it will provide you with ideas from the recipe database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d0c4b1e-b972-4c41-8d14-a94747220972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Bot: Sure, I can provide you with recipes from the dataset. Just let me know which recipe you would like to have more information about.\n",
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Human:  Please give me a recipe with apples\n",
      "Assistant: Sure! Here is a recipe for \"Pear Tart with Honey and Apple Juice\":\n",
      "\n",
      "Ingredients:\n",
      "- 1 sheet of puff pastry\n",
      "- 2 apples\n",
      "- 2 pears\n",
      "- 2 tbsp honey\n",
      "- 100 ml apple juice\n",
      "\n",
      "Instructions:\n",
      "1. Preheat the oven to 200 degrees Celsius.\n",
      "2. Roll out the puff pastry and place it on a baking sheet lined with parchment paper.\n",
      "3. Peel and core the apples and pears, then slice them thinly.\n",
      "4. Arrange the apple and pear slices on the puff pastry, alternating between the two fruits.\n",
      "5. Prick the dough base several times with a fork and fan out the pear slices in an overlapping pattern.\n",
      "6. Mix honey with apple juice and spread it evenly and thinly over the pears.\n",
      "7. Place the pear tart in the cold oven and bake at 200 degrees Celsius for 25 to 30 minutes on the middle rack.\n",
      "8. Serve warm.\n",
      "\n",
      "Enjoy your Pear Tart with Honey and Apple Juice!\n",
      "\n",
      "Human:  do you have a vegan main dish?\n",
      "Assistant: Yes, here is a recipe for a vegan main dish called \"Rhenish-Style Kale\":\n",
      "\n",
      "Ingredients:\n",
      "- 1.5 kg kale\n",
      "- 2 onions\n",
      "- 2 tbsp olive oil\n",
      "- 750 g potatoes\n",
      "- 3 tbsp brown cane sugar\n",
      "- 1/4 l vegetable broth\n",
      "- 3 tbsp mustard\n",
      "- Salt\n",
      "- Black pepper\n",
      "- 4 mettwurst sausages (you can substitute with vegan sausages)\n",
      "\n",
      "Instructions:\n",
      "1. Wash, clean, and chop the kale.\n",
      "2. Peel and roughly dice the onions. Also chop the bacon into cubes.\n",
      "3. Peel the potatoes and cut them into bite-sized cubes.\n",
      "4. Heat oil in a large pot and sauté the onions, bacon, and potatoes in it.\n",
      "5. Add the sugar and let it caramelize while stirring. Then deglaze with the vegetable broth.\n",
      "6. Add the kale and let everything simmer in the covered pot over low heat for 30 minutes.\n",
      "7. 10 minutes before the end of the cooking time, place the vegan sausages on top of the kale to warm them up.\n",
      "8. Season with pepper, salt, and mustard before serving.\n",
      "\n",
      "Enjoy your Rhenish-Style Kale vegan main dish!\n",
      "\n",
      "Human:  is this also gluten-free?\n",
      "Assistant: Yes, the recipe for \"Rhenish-Style Kale\" is both vegan and gluten-free. It does not contain any gluten-containing ingredients, making it suitable for individuals who follow a gluten-free diet. Enjoy your gluten-free and vegan Rhenish-Style Kale main dish!\n",
      "\n",
      "Human:  is there fruit in the rhenish-style kale?\n",
      "Assistant: No, the recipe for \"Rhenish-Style Kale\" does not include any fruit as an ingredient. It primarily consists of kale, onions, potatoes, vegetable broth, mustard, and vegan sausages (or mettwurst sausages). The dish is savory and hearty, with a focus on the flavors of the vegetables and seasonings. Enjoy your fruit-free Rhenish-Style Kale main dish!\n",
      "\n",
      "Human:  exit\n"
     ]
    }
   ],
   "source": [
    "chat_engine = vectorstoreindex.as_chat_engine(chat_mode=\"context\",\n",
    "                                              verbose=True,\n",
    "                                              temperature = 0,\n",
    "                                              system_prompt = \"You are chatbot able to provide recipes. Once i have asked you for a recipe,\"\n",
    "                                              \"please always provide all the information about a recipe in the dataset, including all columns.\",\n",
    "                                              text_qa_template=CHAT_TEXT_QA_PROMPT)\n",
    "chat_engine.reset()\n",
    "initial_user_message = \"Once i have asked you for a recipe, please always provide all the information about a recipe in the dataset, including all columns.\"\n",
    "\n",
    "# Output the model's response\n",
    "print(\"Recipe Bot:\", response)\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35136b-9081-4229-bf46-1a3493d759e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
